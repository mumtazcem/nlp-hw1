{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "files = ['data/conllpp_dev.txt', 'data/conllpp_test.txt', 'data/conllpp_train.txt']\n",
    "#for file in files:\n",
    "#  f = open(file, 'r', encoding='utf-8')\n",
    "#  for line in f.readlines() + ['']:\n",
    "#    print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7daced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(filename):\n",
    "  sentences = []\n",
    "  tokens = []\n",
    "  labels = []\n",
    "  file = open(filename, 'r', encoding='utf-8')\n",
    "  for line in file.readlines() + ['']:\n",
    "    if len(line) == 0 or line.startswith('-DOCSTART-') or line.isspace():\n",
    "      if len(tokens) > 0:\n",
    "        sentences.append((tokens, labels))\n",
    "      tokens = []\n",
    "      labels = []\n",
    "    else:\n",
    "      splits = line.strip().split()\n",
    "      token, label = splits[0], splits[-1]\n",
    "      tokens.append(token)\n",
    "      labels.append(label)\n",
    "      #print(f\"Token: {token}\")\n",
    "      #print(f\"Label: {label}\")\n",
    "  return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed3a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = get_sentences(files[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1702e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences[0][1][0]\n",
    "len(sentences)\n",
    "full_token_list = []\n",
    "full_label_list = []\n",
    "for sentence in sentences:\n",
    "    token_list = sentence[0]\n",
    "    label_list = sentence[1]\n",
    "    full_token_list.extend(token_list)\n",
    "    full_label_list.extend(label_list)\n",
    "print(f\"Token list length: {len(full_token_list)} and label list length: {len(full_label_list)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(list(zip(full_token_list, full_label_list)),\n",
    "               columns =['Tokens', 'Labels'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a06ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_label_list = df.Labels.unique()\n",
    "unique_label_list\n",
    "print(unique_label_list)\n",
    "print(type(unique_label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13de9ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_token_list = df.Tokens.unique()\n",
    "print(len(unique_token_list))\n",
    "print(type(unique_token_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40490a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_counts = df['Labels'].value_counts()\n",
    "labels_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_label_list is --> ['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG',\n",
    "#       'I-MISC', 'I-LOC']\n",
    "print(f\"Label: {unique_label_list[1]} and its count: {labels_counts[unique_label_list[1]]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab00110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A matrix will contain the label transition probabilities. P(T_i, T_i-1)\n",
    "# P(O|B-ORG) = C(B-ORG, O) / C(B-ORG)\n",
    "# Total labels: 203621\n",
    "# Need: P(c_o, c_b_loc), P(c_o, c_b_per), P(c_o, c_b_org) ... for all.\n",
    "# O->B_LOC, O->B_PER etc.\n",
    "import numpy as np\n",
    "\n",
    "# first create 9x9 matrix without <s>\n",
    "a_w_out_start = np.zeros((len(unique_label_list), len(unique_label_list))) # A matrix\n",
    "#print(a)\n",
    "for t_index, t in enumerate(unique_label_list):\n",
    "    for t_minus_1_index, t_minus_1 in enumerate(unique_label_list):\n",
    "        count_t_minus_1_t = 0\n",
    "        for sentence in sentences:\n",
    "            label_list = sentence[1]\n",
    "            for label_index in range(1, len(label_list)):\n",
    "                if label_list[label_index] == t:\n",
    "                    if label_list[label_index - 1] == t_minus_1:\n",
    "                        count_t_minus_1_t += 1\n",
    "        # finished all sentences\n",
    "        probability = count_t_minus_1_t / labels_counts[unique_label_list[t_minus_1_index]]\n",
    "        #print(f\"t_index: {t_index}, t_i-1 index: {t_minus_1_index}\")\n",
    "        a_w_out_start[t_index][t_minus_1_index] = probability\n",
    "\n",
    "print(f\"a_w_out_start: {a_w_out_start}\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "# need start matrix that includes <s> as well.\n",
    "start = np.zeros((1, (len(unique_label_list)))) \n",
    "for t_index, t in enumerate(unique_label_list):\n",
    "    count_t_start = 0\n",
    "    for sentence in sentences:\n",
    "        label_list = sentence[1]\n",
    "        if label_list[0] == t:\n",
    "            count_t_start += 1\n",
    "    probability = count_t_start / labels_counts[unique_label_list[t_index]]\n",
    "    start[0][t_index] = probability\n",
    "print(f\"Only start matrix: {start}\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "# concatanate\n",
    "a = np.concatenate((start, a_w_out_start))\n",
    "print(f\"A matrix: {a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064008c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2126adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B matrix: emmision probabilities\n",
    "# P(w_i|t_i) = C(t_i, w_i) / C(t_i)\n",
    "# P(German|B-MISC) = C(B-MISC, German) / C(B-MISC)\n",
    "# NOTE: Takes a lot of time\n",
    "\n",
    "b = np.zeros((len(unique_token_list), len(unique_label_list))) # B matrix\n",
    "for token_index, token in enumerate(unique_token_list):\n",
    "    for label_index, label in enumerate(unique_label_list):\n",
    "        print(f\"Working on {token_index}. token: {token} and {label_index}. label: {label}\")\n",
    "        count_t_i_w_i = 0\n",
    "        for index, token_input in enumerate(full_token_list):\n",
    "            if token_input == token and label == full_label_list[index]:\n",
    "                count_t_i_w_i += 1\n",
    "        probability = count_t_i_w_i / labels_counts[unique_label_list[label_index]]\n",
    "        b[token_index][label_index] = probability\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fc2265",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('b_matrix.txt', b)\n",
    "b_loaded = np.loadtxt('b_matrix.txt')\n",
    "b_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24848382",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "np.savetxt('b_matrix_scientific.txt', b)\n",
    "b_loaded_sci = np.loadtxt('b_matrix_scientific.txt')\n",
    "b_loaded_sci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9684017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.transpose(b)\n",
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe600e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaffe817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token np array to list\n",
    "unique_token_list_lower = []\n",
    "for token in unique_token_list:\n",
    "    unique_token_list_lower.append(token.lower())\n",
    "try:\n",
    "    token_index = unique_token_list_lower.index(\"japan\")\n",
    "    print(ind)\n",
    "except Exception as exc:\n",
    "    print(f\"exception: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd7762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Algorithm\n",
    "# given input sentence: Janet will back the bill\n",
    "# word_array, T = ['Janet', 'will', 'back', 'the', 'bill']\n",
    "# unique_label_list == N\n",
    "def viterbi(N, T):  \n",
    "    viterbi_matrix = np.zeros((len(N), len(T))) # B matrix\n",
    "    backpointer = np.zeros((len(N), len(T)))\n",
    "    for s_index, s in enumerate(N):\n",
    "        try:\n",
    "            item_index = unique_token_list_lower.index(T[0].lower())\n",
    "        except Exception as exc:\n",
    "            #raise Exception(f\"Item {T[0].lower()} is not found in the token list. Exception: {exc}\")\n",
    "            item_index = -1\n",
    "        # if the word is found in B matrix \n",
    "        if item_index > -1:\n",
    "            viterbi_matrix[s_index][0] = a[0][s_index] * B[s_index][item_index]\n",
    "        else:\n",
    "            viterbi_matrix[s_index][0] = 0\n",
    "        backpointer[s_index][0] = 0\n",
    "    # initialization ends. \n",
    "    # recursion step\n",
    "    for t_index, t in enumerate(T[1:]):\n",
    "        for s_index, s in enumerate(N):\n",
    "            # maximum probability viterbi step\n",
    "            max_prob = 0.0\n",
    "            for s_prime_index, s_prime in enumerate(N):\n",
    "                try:\n",
    "                    item_index = unique_token_list_lower.index(T[t_index + 1].lower())\n",
    "                except Exception as exc:\n",
    "                    #raise Exception(f\"Item {T[t_index].lower()} is not found in the token list. Exception: {exc}\")\n",
    "                    item_index = -1\n",
    "                # if the word is found in B matrix\n",
    "                if item_index > -1:\n",
    "                    b_s_o_t = B[s_index][item_index]\n",
    "                else:\n",
    "                    b_s_o_t = 0\n",
    "                calculated_probability = viterbi_matrix[s_prime_index][t_index] * a_w_out_start[s_prime_index][s_index] * b_s_o_t # it is not t_index - 1 since enumerate make index start with 0\n",
    "                if calculated_probability > max_prob:\n",
    "                    max_prob = calculated_probability\n",
    "            viterbi_matrix[s_index][t_index + 1] = max_prob # # it is t_index + 1 since enumerate make index start with 0\n",
    "            backpointer[s_index][t_index + 1] = max_prob # it is t_index + 1 since enumerate make index start with 0\n",
    "            # maximum probability viterbi step ends\n",
    "    # find bestpathprob & best_state_dict\n",
    "    best_state_list = []\n",
    "    best_prob_list = []\n",
    "    for t_index, t in enumerate(T):\n",
    "        best_state = 0.0\n",
    "        best_state_list.append(\"Not_Assigned\")\n",
    "        best_prob_list.append(best_state)\n",
    "        for s_index, s in enumerate(N):\n",
    "            if viterbi_matrix[s_index][t_index] > best_state:\n",
    "                best_state = viterbi_matrix[s_index][t_index]\n",
    "                best_prob_list[t_index] = viterbi_matrix[s_index][t_index]\n",
    "                best_state_list[t_index] = s\n",
    "        if best_state == 0.0:\n",
    "            best_state_list[t_index] = \"O\" # Assumption: Token is outside of any span of interest, thus mark it as O\n",
    "    return best_prob_list, best_state_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c697194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOCCER NN I-NP O\n",
    "#- : O O\n",
    "#JAPAN NNP I-NP B-LOC\n",
    "#GET VB I-VP O\n",
    "#LUCKY NNP I-NP O\n",
    "#WIN NNP I-NP O\n",
    "#, , O O\n",
    "#CHINA NNP I-NP B-LOC\n",
    "#IN IN I-PP O\n",
    "#SURPRISE DT I-NP O\n",
    "#DEFEAT NN I-NP O\n",
    "#. . O O\n",
    "np.set_printoptions(suppress=False)\n",
    "T = ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']\n",
    "best_prob_dict, best_state_dict = viterbi(unique_label_list, T)\n",
    "\n",
    "print(f\"best_prob_dict is: {best_prob_dict}\")\n",
    "print(f\"best_state_dict is: {best_state_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebfc4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test file\n",
    "test_sentences = get_sentences(files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff20d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(test_sentences)\n",
    "test_token_list = []\n",
    "test_label_list = []\n",
    "for sentence in test_sentences:\n",
    "    token_list = sentence[0]\n",
    "    label_list = sentence[1]\n",
    "    test_token_list.extend(token_list)\n",
    "    test_label_list.extend(label_list)\n",
    "print(f\"Test token list length: {len(test_token_list)} and test label list length: {len(test_label_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8068f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test = pd.DataFrame(list(zip(test_token_list, test_label_list)),\n",
    "               columns =['Tokens', 'Labels'])\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c4a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_counts_test = df_test['Labels'].value_counts()\n",
    "labels_counts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c4284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert label np array to list\n",
    "unique_label_python_list = []\n",
    "for label in unique_label_list:\n",
    "    unique_label_python_list.append(label)\n",
    "try:\n",
    "    label_index = unique_label_python_list.index(\"B-LOC\")\n",
    "    print(label_index)\n",
    "except Exception as exc:\n",
    "    print(f\"exception: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = np.zeros((len(unique_label_list), len(unique_label_list)))\n",
    "for sentence in test_sentences:\n",
    "    token_list = sentence[0]\n",
    "    label_list = sentence[1]\n",
    "    #print(token_list)\n",
    "    best_prob_list, prediction_list = viterbi(unique_label_list, token_list)\n",
    "    #print(f\"best_prob_list: {best_prob_list}\")\n",
    "    #print(f\"Prediction values: {prediction_list}, len: {len(prediction_list)}\")\n",
    "    #print(f\"Y-test values: {label_list}, len: {len(label_list)}\")\n",
    "    for index, y_item in enumerate(label_list):\n",
    "        y_index = unique_label_python_list.index(y_item) # locate y_test value in confusion matrix\n",
    "        prediction_index = unique_label_python_list.index(prediction_list[index]) # locate y_pred value in confusion matrix\n",
    "        confusion_matrix[prediction_index][y_index] += 1\n",
    "        #print(f\"prediction_index is {prediction_index}, y_index is {y_index}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b18ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create matrix for each label with corresponding precision recall f1-score\n",
    "eval_matrix = np.zeros((len(unique_label_list), 3))\n",
    "tp_total = 0\n",
    "tn_total = 0\n",
    "fp_total = 0\n",
    "fn_total = 0\n",
    "precision_total = 0\n",
    "recall_total = 0\n",
    "f1_total = 0\n",
    "for index, label in enumerate(unique_label_list):\n",
    "    tp = confusion_matrix[index][index]\n",
    "    tn_matrix = numpy.delete(confusion_matrix, (index), axis=0)\n",
    "    tn_matrix = numpy.delete(tn_matrix, (index), axis=1)\n",
    "    tn = np.sum(tn_matrix)\n",
    "    fp = np.sum(confusion_matrix[index]) - tp\n",
    "    fn = np.sum(confusion_matrix[ : , index]) - tp\n",
    "    tp_total += tp\n",
    "    tn_total += tn\n",
    "    fp_total += fp\n",
    "    fn_total += fn\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * tp / (2 * tp + fp + fn)\n",
    "    precision_total += precision\n",
    "    recall_total += recall\n",
    "    f1_total += f1\n",
    "    eval_matrix[index][0] = precision\n",
    "    eval_matrix[index][1] = recall\n",
    "    eval_matrix[index][2] = f1\n",
    "    \n",
    "print(f\"Aşağıdaki matris için verilen etiketler (yukarıdan aşağıya): \\n{np.transpose(unique_label_list)}\")  \n",
    "print(f\"  kesinlik   duyarlılık       f1 \\n{eval_matrix}\")\n",
    "micro_precision = tp_total / (tp_total + fp_total)\n",
    "micro_recall = tp_total / (tp_total + fn_total)\n",
    "micro_f1 = 2 * tp_total / (2 * tp_total + fp_total + fn_total) \n",
    "print(f\"           kesinlik                duyarlılık                    f1\")\n",
    "print(f\"Mikro ort: {micro_precision}       {micro_recall}        {micro_f1}\")   \n",
    "print(f\"Makro ort: {precision_total/len(unique_label_list)}       {recall_total/len(unique_label_list)}       {f1_total/len(unique_label_list)}\")   \n",
    "\n",
    "weighted_precision_total = 0\n",
    "weighted_recall_total = 0\n",
    "weighted_f1_total = 0\n",
    "for index, label in enumerate(unique_label_list):\n",
    "    weighted_precision_total += eval_matrix[index][0] * labels_counts_test[label]\n",
    "    weighted_recall_total += eval_matrix[index][1] * labels_counts_test[label]\n",
    "    weighted_f1_total += eval_matrix[index][2] * labels_counts_test[label]\n",
    "print(f\"Ağırlıklı F1: {weighted_precision_total/np.sum(labels_counts_test)}       {weighted_recall_total/np.sum(labels_counts_test)}       {weighted_f1_total/np.sum(labels_counts_test)}\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b757ab2c",
   "metadata": {},
   "source": [
    "# 2. Dereceden HMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba087f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# HMM 2. degree\n",
    "# A matrix will contain the label transition probabilities. P(T_i | T_i-1, T_i-2)\n",
    "\n",
    "# 3d matrix:\n",
    "a_second_w_out_start = np.zeros((len(unique_label_list), len(unique_label_list), len(unique_label_list))) # 3d A matrix\n",
    "\n",
    "for t_index, t in enumerate(unique_label_list):\n",
    "    for t_minus_1_index, t_minus_1 in enumerate(unique_label_list):\n",
    "        for t_minus_2_index, t_minus_2 in enumerate(unique_label_list):\n",
    "            # first count C(T_i-1, T_i-2)\n",
    "            count_t_i_minus_1_and_t_i_minus_2 = 0\n",
    "            for sentence in sentences:\n",
    "                label_list = sentence[1]\n",
    "                for label_index in range(1, len(label_list)):\n",
    "                    if label_list[label_index - 1] == t_minus_1:\n",
    "                        if label_list[label_index - 2] == t_minus_2:\n",
    "                            count_t_i_minus_1_and_t_i_minus_2 += 1\n",
    "            \n",
    "            # then count C(T_i-1, T_i-2, T_i)\n",
    "            count_trigram = 0\n",
    "            for sentence in sentences:\n",
    "                label_list = sentence[1]\n",
    "                for label_index in range(1, len(label_list)):\n",
    "                    if label_list[label_index] == t:\n",
    "                        if label_list[label_index - 1] == t_minus_1:\n",
    "                            if label_list[label_index - 2] == t_minus_2:\n",
    "                                count_trigram += 1\n",
    "            # finished all sentences\n",
    "            if count_t_i_minus_1_and_t_i_minus_2 == 0:\n",
    "                #print('Division by zero!')\n",
    "                probability = 0.0\n",
    "            else:\n",
    "                probability = count_trigram / count_t_i_minus_1_and_t_i_minus_2 \n",
    "            #print(f\"t_index: {t_index}, t_i-1 index: {t_minus_1_index}\")\n",
    "            a_second_w_out_start[t_index, t_minus_1_index, t_minus_2_index] = probability\n",
    "                \n",
    "\n",
    "print(f\"a_second_w_out_start: {a_second_w_out_start}\")\n",
    "print(\"-----------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Algorithm\n",
    "# given input sentence: Janet will back the bill\n",
    "# word_array, T = ['Janet', 'will', 'back', 'the', 'bill']\n",
    "# unique_label_list == N\n",
    "def viterbi_2d(N, T):  \n",
    "    viterbi_matrix = np.zeros((len(N), len(T))) # B matrix\n",
    "    backpointer = np.zeros((len(N), len(T)))\n",
    "    for s_index, s in enumerate(N):\n",
    "        try:\n",
    "            item_index = unique_token_list_lower.index(T[0].lower())\n",
    "        except Exception as exc:\n",
    "            #raise Exception(f\"Item {T[0].lower()} is not found in the token list. Exception: {exc}\")\n",
    "            item_index = -1\n",
    "        # if the word is found in B matrix \n",
    "        if item_index > -1:\n",
    "            viterbi_matrix[s_index][0] = a[0][s_index] * B[s_index][item_index]\n",
    "        else:\n",
    "            viterbi_matrix[s_index][0] = 0\n",
    "        backpointer[s_index][0] = 0\n",
    "        \n",
    "    # initialization ends. \n",
    "    # recursion step\n",
    "    for t_index, t in enumerate(T[1:]):\n",
    "        # second col of viterbi matrix\n",
    "        if t_index == 0:\n",
    "            for s_index, s in enumerate(N):\n",
    "                # maximum probability viterbi step\n",
    "                max_prob = 0.0\n",
    "                for s_prime_index, s_prime in enumerate(N):\n",
    "                    try:\n",
    "                        item_index = unique_token_list_lower.index(T[t_index + 1].lower())\n",
    "                    except Exception as exc:\n",
    "                        #raise Exception(f\"Item {T[t_index].lower()} is not found in the token list. Exception: {exc}\")\n",
    "                        item_index = -1\n",
    "                    # if the word is found in B matrix\n",
    "                    if item_index > -1:\n",
    "                        b_s_o_t = B[s_index][item_index]\n",
    "                    else:\n",
    "                        b_s_o_t = 0\n",
    "                    calculated_probability = viterbi_matrix[s_prime_index][t_index] * a_w_out_start[s_prime_index][s_index] * b_s_o_t # it is not t_index - 1 since enumerate make index start with 0\n",
    "                    if calculated_probability > max_prob:\n",
    "                        max_prob = calculated_probability\n",
    "                viterbi_matrix[s_index][t_index + 1] = max_prob # # it is t_index + 1 since enumerate make index start with 0\n",
    "                backpointer[s_index][t_index + 1] = max_prob # it is t_index + 1 since enumerate make index start with 0\n",
    "                # maximum probability viterbi step ends\n",
    "        # after second col\n",
    "        else:\n",
    "            # find  the best path at t_index - 1\n",
    "            \n",
    "            best_state = 0.0\n",
    "            best_label = \"\"\n",
    "            for s_index, s in enumerate(N):\n",
    "                if viterbi_matrix[s_index][t_index - 1] > best_state:\n",
    "                    best_state = viterbi_matrix[s_index][t_index - 1]\n",
    "                    best_label = s\n",
    "            if best_state == 0.0:\n",
    "                best_label = \"O\" \n",
    "                \n",
    "            best_label_index = unique_label_python_list.index(best_label) # best label index at t-2   \n",
    "            for s_index, s in enumerate(N):\n",
    "                # maximum probability viterbi step\n",
    "                max_prob = 0.0\n",
    "                for s_prime_index, s_prime in enumerate(N):\n",
    "                    try:\n",
    "                        item_index = unique_token_list_lower.index(T[t_index + 1].lower())\n",
    "                    except Exception as exc:\n",
    "                        #raise Exception(f\"Item {T[t_index].lower()} is not found in the token list. Exception: {exc}\")\n",
    "                        item_index = -1\n",
    "                    # if the word is found in B matrix\n",
    "                    if item_index > -1:\n",
    "                        b_s_o_t = B[s_index][item_index]\n",
    "                    else:\n",
    "                        b_s_o_t = 0\n",
    "                    calculated_probability = viterbi_matrix[s_prime_index][t_index] * a_second_w_out_start[s_prime_index][s_index][best_label_index] * b_s_o_t # it is not t_index - 1 since enumerate make index start with 0\n",
    "                    if calculated_probability > max_prob:\n",
    "                        max_prob = calculated_probability\n",
    "                viterbi_matrix[s_index][t_index + 1] = max_prob # # it is t_index + 1 since enumerate make index start with 0\n",
    "                backpointer[s_index][t_index + 1] = max_prob # it is t_index + 1 since enumerate make index start with 0\n",
    "                # maximum probability viterbi step ends\n",
    "    # find bestpathprob & best_state_dict\n",
    "    best_state_list = []\n",
    "    best_prob_list = []\n",
    "    for t_index, t in enumerate(T):\n",
    "        best_state = 0.0\n",
    "        best_state_list.append(\"Not_Assigned\")\n",
    "        best_prob_list.append(best_state)\n",
    "        for s_index, s in enumerate(N):\n",
    "            if viterbi_matrix[s_index][t_index] > best_state:\n",
    "                best_state = viterbi_matrix[s_index][t_index]\n",
    "                best_prob_list[t_index] = viterbi_matrix[s_index][t_index]\n",
    "                best_state_list[t_index] = s\n",
    "        if best_state == 0.0:\n",
    "            best_state_list[t_index] = \"O\" # Assumption: Token is outside of any span of interest, thus mark it as O\n",
    "    return best_prob_list, best_state_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b214fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOCCER NN I-NP O\n",
    "#- : O O\n",
    "#JAPAN NNP I-NP B-LOC\n",
    "#GET VB I-VP O\n",
    "#LUCKY NNP I-NP O\n",
    "#WIN NNP I-NP O\n",
    "#, , O O\n",
    "#CHINA NNP I-NP B-LOC\n",
    "#IN IN I-PP O\n",
    "#SURPRISE DT I-NP O\n",
    "#DEFEAT NN I-NP O\n",
    "#. . O O\n",
    "np.set_printoptions(suppress=False)\n",
    "T = ['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']\n",
    "best_prob_dict, best_state_dict = viterbi_2d(unique_label_list, T)\n",
    "\n",
    "print(f\"best_prob_dict is: {best_prob_dict}\")\n",
    "print(f\"best_state_dict is: {best_state_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e500407",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = np.zeros((len(unique_label_list), len(unique_label_list)))\n",
    "for sentence in test_sentences:\n",
    "    token_list = sentence[0]\n",
    "    label_list = sentence[1]\n",
    "    #print(token_list)\n",
    "    best_prob_list, prediction_list = viterbi_2d(unique_label_list, token_list)\n",
    "    #print(f\"best_prob_list: {best_prob_list}\")\n",
    "    #print(f\"Prediction values: {prediction_list}, len: {len(prediction_list)}\")\n",
    "    #print(f\"Y-test values: {label_list}, len: {len(label_list)}\")\n",
    "    for index, y_item in enumerate(label_list):\n",
    "        y_index = unique_label_python_list.index(y_item) # locate y_test value in confusion matrix\n",
    "        prediction_index = unique_label_python_list.index(prediction_list[index]) # locate y_pred value in confusion matrix\n",
    "        confusion_matrix[prediction_index][y_index] += 1\n",
    "        #print(f\"prediction_index is {prediction_index}, y_index is {y_index}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd960a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create matrix for each label with corresponding precision recall f1-score\n",
    "eval_matrix = np.zeros((len(unique_label_list), 3))\n",
    "tp_total = 0\n",
    "tn_total = 0\n",
    "fp_total = 0\n",
    "fn_total = 0\n",
    "precision_total = 0\n",
    "recall_total = 0\n",
    "f1_total = 0\n",
    "for index, label in enumerate(unique_label_list):\n",
    "    tp = confusion_matrix[index][index]\n",
    "    tn_matrix = numpy.delete(confusion_matrix, (index), axis=0)\n",
    "    tn_matrix = numpy.delete(tn_matrix, (index), axis=1)\n",
    "    tn = np.sum(tn_matrix)\n",
    "    fp = np.sum(confusion_matrix[index]) - tp\n",
    "    fn = np.sum(confusion_matrix[ : , index]) - tp\n",
    "    tp_total += tp\n",
    "    tn_total += tn\n",
    "    fp_total += fp\n",
    "    fn_total += fn\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * tp / (2 * tp + fp + fn)\n",
    "    precision_total += precision\n",
    "    recall_total += recall\n",
    "    f1_total += f1\n",
    "    eval_matrix[index][0] = precision\n",
    "    eval_matrix[index][1] = recall\n",
    "    eval_matrix[index][2] = f1\n",
    "    \n",
    "print(f\"Aşağıdaki matris için verilen etiketler (yukarıdan aşağıya): \\n{np.transpose(unique_label_list)}\")  \n",
    "print(f\"  kesinlik   duyarlılık       f1 \\n{eval_matrix}\")\n",
    "micro_precision = tp_total / (tp_total + fp_total)\n",
    "micro_recall = tp_total / (tp_total + fn_total)\n",
    "micro_f1 = 2 * tp_total / (2 * tp_total + fp_total + fn_total) \n",
    "print(f\"           kesinlik                duyarlılık                    f1\")\n",
    "print(f\"Mikro ort: {micro_precision}       {micro_recall}        {micro_f1}\")   \n",
    "print(f\"Makro ort: {precision_total/len(unique_label_list)}       {recall_total/len(unique_label_list)}       {f1_total/len(unique_label_list)}\")   \n",
    "\n",
    "weighted_precision_total = 0\n",
    "weighted_recall_total = 0\n",
    "weighted_f1_total = 0\n",
    "for index, label in enumerate(unique_label_list):\n",
    "    weighted_precision_total += eval_matrix[index][0] * labels_counts_test[label]\n",
    "    weighted_recall_total += eval_matrix[index][1] * labels_counts_test[label]\n",
    "    weighted_f1_total += eval_matrix[index][2] * labels_counts_test[label]\n",
    "print(f\"Ağırlıklı F1: {weighted_precision_total/np.sum(labels_counts_test)}       {weighted_recall_total/np.sum(labels_counts_test)}       {weighted_f1_total/np.sum(labels_counts_test)}\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d1c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
